{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aba1a21-28cd-41dd-b140-cb191deb2cbb",
   "metadata": {},
   "source": [
    "## Deploy Llama 3.2 through vLLM on SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d2cb7e-3c99-4e0b-9764-c70e61c5fbd7",
   "metadata": {},
   "source": [
    "원본 코드\n",
    "- https://github.com/aws-samples/aws-ai-ml-workshop-kr/blob/master/genai/aws-gen-ai-kr/40_inference/31-Llama-3-1-Inference/1_llama-3-1-deploy-djl-lmi.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ba69ac-e772-480e-a0a3-44199ebe9a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de2aa51-1507-4b24-a828-8e00a5910cd6",
   "metadata": {},
   "source": [
    "### 1. Depoly model on SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c732aa-0d20-4fc7-b571-eb6f3774f3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381515c4-e43b-4099-b040-38c97fe5b3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "sm_client = boto3.client(\"sagemaker\", region_name=region)\n",
    "sm_runtime_client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9dfd85-8076-4222-96a2-056b773fe8fe",
   "metadata": {},
   "source": [
    "#### 1-1. Setup Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfa35fd-0110-43f0-b0a8-48fabf09b222",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5980803f-6593-4c03-865b-a6251fab7bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "container_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"djl-lmi\", version=\"0.29.0\", region=region\n",
    ")\n",
    "container_uri\n",
    "# container_uri = \"763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc24e2e-7a10-4a21-a8e4-481ee9d2630a",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.g5.8xlarge\"\n",
    "# instance_type = \"ml.g5.12xlarge\"\n",
    "container_startup_health_check_timeout = 600\n",
    "\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"Meta-Llama-3-1-8B-Instruct\")\n",
    "\n",
    "print (f'container_uri: {container_uri}')\n",
    "print (f'container_startup_health_check_timeout: {container_startup_health_check_timeout}')\n",
    "print (f'instance_type: {instance_type}')\n",
    "print (f'endpoint_name: {endpoint_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dd8509-e87f-44d8-a416-d745c9dacbb0",
   "metadata": {},
   "source": [
    "#### 1-2. Creat model with env variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d798365-7533-488d-97bd-33d861a97670",
   "metadata": {},
   "source": [
    "사용 가능 옵션\n",
    "- https://github.com/deepjavalibrary/djl-serving/tree/master/serving/docs/lmi\n",
    "\n",
    "“vllm” 옵션 선택 시 paged attention은 default 적용.\n",
    "- vLLM env var. 를 이용해서 Backend 셋팅 가능 (e.g., VLLM_ATTENTION_BACKEND\")\n",
    "- https://docs.vllm.ai/en/latest/serving/env_vars.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f02dfed-e0b7-4b6f-8e74-2606f203943c",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = \"[YOUR TOKEN]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13facca-de21-4f99-ad0d-1d8c7a2ac432",
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_env = {\n",
    "    \"HF_MODEL_ID\": model_id,\n",
    "    \"OPTION_ROLLING_BATCH\": \"vllm\",\n",
    "    \"OPTION_TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "    \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"2\",\n",
    "    \"OPTION_DTYPE\":\"fp16\",\n",
    "    \"OPTION_MAX_MODEL_LEN\": \"1024\",\n",
    "    # \"VLLM_ATTENTION_BACKEND\": \"XFORMERS\",\n",
    "    \"HF_TOKEN\": HF_TOKEN\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e91ae8-847c-4a07-b5c0-922b76fc8e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sagemaker.Model(\n",
    "    image_uri=container_uri, \n",
    "    role=role,\n",
    "    env=deploy_env,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba20fb9-a363-4b09-8e8f-230d9eaca60a",
   "metadata": {},
   "source": [
    "#### 1-3. Deploy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fe50b3-ba70-4d55-b4ff-8365a622decc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.deploy(\n",
    "    instance_type=instance_type,\n",
    "    initial_instance_count=1,\n",
    "    endpoint_name=endpoint_name,\n",
    "    container_startup_health_check_timeout=container_startup_health_check_timeout\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe44c6d-8033-42ae-ad9f-a0ee496c6a78",
   "metadata": {},
   "source": [
    "### 2. Invocation (Generate Text using the endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ec24a0-5a52-4c53-9822-8de55ab586e8",
   "metadata": {},
   "source": [
    "#### 2-1. Get a predictor for your endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e0c192-971c-4b85-afa9-c586e622bb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    serializer=sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8f0a2e-969f-4d43-a63d-45c6f427cc74",
   "metadata": {},
   "source": [
    "#### 2-1. Make a prediction with your endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51471c27-192b-471b-ad99-be8caded5840",
   "metadata": {},
   "source": [
    "- Normal Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778b7f71-9ac5-413a-b3c5-7c9bd8d6554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = predictor.predict(\n",
    "    {\n",
    "        \"inputs\": \"write a quick sort algorithm in python and description\",\n",
    "        \"parameters\": {\"do_sample\": True, \"max_new_tokens\": 2048},\n",
    "    }\n",
    ")\n",
    "\n",
    "print(outputs[\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1a58a9-05c2-4890-b37e-ad3997a2cf98",
   "metadata": {},
   "source": [
    "- Chat template\n",
    "    - [DJL Chat Completions API Schema](https://docs.djl.ai/master/docs/serving/serving/docs/lmi/user_guides/chat_input_output_schema.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab9dab7-bdd1-4829-aada-af87f1c000d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works! anyway, write a quick sort algorithm in python and description\"},\n",
    "]\n",
    "\n",
    "result = predictor.predict(\n",
    "    {\"messages\": chat, \"max_tokens\": 1024}\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a2ceec-1e48-4a0a-8725-d3a49dbc9aed",
   "metadata": {},
   "source": [
    "### 3. Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb4327b-226b-4c6d-8aa1-1476cb0909dc",
   "metadata": {},
   "source": [
    "#### 3-1. Normal Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71020310-4228-4205-b770-5e7349b2e52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67adc71-d3b6-42ba-aada-a9dbc794846d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다양한 코딩 태스크를 위한 프롬프트 리스트\n",
    "prompts = [\n",
    "    \"write a quick sort algorithm in python.\",\n",
    "    \"Write a Python function to implement a binary search algorithm.\",\n",
    "    \"Create a JavaScript function to flatten a nested array.\",\n",
    "    \"Implement a simple REST API using Flask in Python.\",\n",
    "    \"Write a SQL query to find the top 5 customers by total purchase amount.\",\n",
    "    \"Create a React component for a todo list with basic CRUD operations.\",\n",
    "    \"Implement a depth-first search algorithm for a graph in C++.\",\n",
    "    \"Write a bash script to find and delete files older than 30 days.\",\n",
    "    \"Create a Python class to represent a deck of cards with shuffle and deal methods.\",\n",
    "    \"Write a regular expression to validate email addresses.\",\n",
    "    \"Implement a basic CI/CD pipeline using GitHub Actions.\"\n",
    "]\n",
    "\n",
    "def generate_payload():\n",
    "    # 랜덤하게 프롬프트 선택\n",
    "    prompt = random.choice(prompts)\n",
    "    \n",
    "    # JSON 페이로드 생성\n",
    "    body = {\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": 400,\n",
    "            # \"return_full_text\": False  # This does not work with Phi3\n",
    "        },\n",
    "        \"stream\": True,\n",
    "    }\n",
    "    \n",
    "    # JSON을 문자열로 변환하고 bytes로 인코딩\n",
    "    return json.dumps(body).encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12010dfe-2e58-499d-a58a-f7bf4b8eaf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Invoke the endpoint\n",
    "resp = sm_runtime_client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name, \n",
    "    Body=generate_payload(), \n",
    "    ContentType=\"application/json\"\n",
    ")\n",
    "print(\"Generated response:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "buffer = \"\"\n",
    "for event in resp['Body']:\n",
    "    if 'PayloadPart' in event:\n",
    "        chunk = event['PayloadPart']['Bytes'].decode()\n",
    "        buffer += chunk\n",
    "        try:\n",
    "            # Try to parse the buffer as JSON\n",
    "            data = json.loads(buffer)\n",
    "            if 'token' in data:\n",
    "                print(data['token']['text'], end='', flush=True)\n",
    "            buffer = \"\"  # Clear the buffer after successful parsing\n",
    "        except json.JSONDecodeError:\n",
    "            # If parsing fails, keep the buffer for the next iteration\n",
    "            pass\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176b5121-c871-4b04-b819-a795bc4de873",
   "metadata": {},
   "source": [
    "#### 3-2. With chat template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56376018-af25-4f7d-aed0-3f1c2cae5dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다양한 코딩 태스크를 위한 프롬프트 리스트\n",
    "chat = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works! anyway, write a quick sort algorithm in python and description\"},\n",
    "]\n",
    "\n",
    "result = predictor.predict(\n",
    "    {\"messages\": chat, \"max_tokens\": 1024}\n",
    ")\n",
    "\n",
    "def generate_payload():\n",
    "    # 랜덤하게 프롬프트 선택\n",
    "    prompt = random.choice(prompts)\n",
    "    \n",
    "    # JSON 페이로드 생성\n",
    "    body = {\n",
    "        \"messages\": chat,\n",
    "        \"max_tokens\": 1024,\n",
    "        \"stream\": True,\n",
    "    }\n",
    "    \n",
    "    # JSON을 문자열로 변환하고 bytes로 인코딩\n",
    "    return json.dumps(body).encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7654a77-044d-4775-99e6-d25dce618a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Invoke the endpoint\n",
    "resp = sm_runtime_client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name, \n",
    "    # Body=json.dumps(body), \n",
    "    Body=generate_payload(), \n",
    "    \n",
    "    ContentType=\"application/json\"\n",
    ")\n",
    "print(\"Generated response:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "buffer = \"\"\n",
    "for event in resp['Body']:\n",
    "    if 'PayloadPart' in event:\n",
    "        chunk = event['PayloadPart']['Bytes'].decode()\n",
    "        buffer += chunk\n",
    "        try:\n",
    "            # Try to parse the buffer as JSON\n",
    "            data = json.loads(buffer)\n",
    "            if 'choices' in data:\n",
    "                print(data['choices'][0]['delta']['content'], end='', flush=True)\n",
    "            buffer = \"\"  # Clear the buffer after successful parsing\n",
    "        except json.JSONDecodeError:\n",
    "            # If parsing fails, keep the buffer for the next iteration\n",
    "            pass\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9814a82",
   "metadata": {},
   "source": [
    "## 4. SageMaker Endpoint 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518cb3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor.delete_predictor()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
